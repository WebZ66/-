# 文件上传



## 单文件上传

实现思路：创建一个input元素，将即type设置为file。

```js
            const input = document.querySelector('input')
            input.onchange = function (e) {
                console.log(e.target.files)
            }
```

通过onchange事件监听上传，并通过e.target.files获取文件对象



## 多文件上传

**只需要给input元素添加个multiple属性即可**

```html
     <input type="file" multiple />
```



## 拖拽上传

- input元素自身其实就是支持拖拽上传的，只需要让其宽高百分百，撑满容器，然后不透明度设置为0即可
- 让容器称为一个拖拽的存放容器

```js
            const div = document.querySelector('div')
            div.ondragover = function (e) {
                e.preventDefault()
                //阻止div元素成为拖拽过程中容器
            }
            div.ondrop = function (e) {
                e.preventDefault() //阻止打开新界面
                console.log(e.dataTransfer.files)
            }
```

![image-20231222144249343](https://gitee.com/zhengdashun/pic_bed/raw/master/img/image-20231222144249343.png) 



## 大文件分片上传

文件对象file其实是特殊的Blob对象，实现分片其实就是通过slice方法，对blob对象进行分块。

```js
            function createChunks(file, chunkSize) {
                const result = []
                for (let i = 0; i < file.size; i += chunkSize) {
                    result.push(file.slice(i, i + chunkSize))
                }
                return result
            }
```

分块完成以后，如果我们直接传递给后端，那么后端如何判断这些块属于哪个文件，因此需要生成一个`唯一的文件标识`。为此，我们需要导入spark-md5.js，生成文件的hash值。好处是，只要文件任何一处发生改变，那么hash值也会变化。

但是通过md5生成hash值，需要一定的时间，如果文件过大，会导致用户体验很差。因此开启一个webwoker。

```js
self.importScripts('../spark-md5.js')


//接收到主线程传递的分块内容
self.onmessage = function (event) {
    hash(event.data).then((v) => {
        self.postMessage(v)
    })
    function hash(chunks) {
        return new Promise((resolve) => {
            const spark = new SparkMD5()
            function _read(i) {
                if (i >= chunks.length) {
                    //读取了所有的块 需要调用spark.end(),返回的hash值
                    resolve(spark.end())
                    return
                }
                const reader = new FileReader()
                //通过readAsArrayBuffer读取其二进制数据，然后进行加密
                reader.readAsArrayBuffer(chunks[i])
                reader.onload = function (e) {
                    //读取到了对应块的字节
                    console.log('e', e)
                    let bytes = e.target.result
                    spark.append(bytes)
                    _read(i + 1)
                }
            }
            _read(0)
        })
    }
}

```



## 秒传

- 上传文件前，先计算出文件的hash值，将文件hash值和文件名发送给服务器
- 服务器接收到后，判断该文件是否已经接收成功
- 如果接收成功，就直接提示上传成功。如果没接收成功，重新进行上传操作

**因此，秒传的本质是`不传`**



## **断点续传**

- 上传文件前，先计算出文件的hash值，将文件hash值和文件名发送给服务器
- 服务器接收到后，首先判断文件是否存在，存在就秒传
- 如果文件不存在，查找分片数据，将已经存在的分片名返回给客户端
- 客户端重新上传时，遍历文件分块，可以判断是否已经存在于后端的分片数据中，如果已经存在，就可以直接跳过，如果没有，再重新上传分片





## 上传进度监控

